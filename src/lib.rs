use anyhow::Result;
use futures::stream::Stream;
use futures::FutureExt;
use reqwest::IntoUrl;
use std::collections::{HashMap, HashSet, VecDeque};
use std::fmt;
use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};

mod domain;
pub mod error;

pub use domain::{AllowList, AllowListConfig, BlockList, DomainListing};

/// Reexport all the scraper types
pub trait Scraper {}

/// Crawler è´Ÿè´£æ‰§è¡Œéœ€è¦çˆ¬å–çš„ç½‘ç«™åœ°å€åˆ—è¡¨, 
/// å¹¶å°†æˆåŠŸçš„å“åº”ç»“æœè½¬å‘ç»™ Scraper å¤„ç†
pub struct Crawler<T: Scraper> {
    client: reqwest::Client,
    /// åŸŸååˆ—è¡¨ï¼šé»‘åå–®èˆ‡ç™½åå–®
    lsit: DomainListing<T::State>,
    /// è¿½è¹¤å·²æäº¤è«‹æ±‚çš„æ·±åº¦
    current_depth: usize,
    /// The maximum depth reqwest are allowed to next.
    max_depth: usize,
    /// æ˜¯å¦éµå¾ªç›®æ¨™ç¶²ç«™çˆ¬èŸ²è¦å‰‡
    respect_robots_txt: bool,
    /// æ˜¯å¦å¿½ç•¥æœªæˆåŠŸè«‹æ±‚éŸ¿æ‡‰
    skip_non_successful_response: bool,
}

impl<T: Scraper> Crawler<T> {
    /// ä» CrawlerConfig ä¸­åˆ›å»º Crawler å®ä¾‹
    pub fn new(config: CrawlerConfig) -> Self {
        let client = config.client.unwrap_or_default(); 

        let list = if config.allow_domains.is_empty() {
            let block_list = BlockList::new(
                config.disallowed_domain,
                client.clone(),
                config.respect_robots_txt,
                config.skip_non_successful_response,
                config.max_depth.unwrap_or(usize::MAX),
                config
                    .max_requests
                    .unwrap_or(CrawlerConfig::MAX_CONCURRENT_REQUESTS),
            );

            DomainListing::BlockList(block_list)
        } else {
            let mut allow_list = AllowList::Default();
            let max_requests = config
                .max_requests
                .unwrap_or(CrawlerConfig::MAX_CONCURRENT_REQUESTS)
                / config.allowed_domains.len();
            for (domain, delay) in config.allowed_domains {
                let allow = AllowListConfig {
                    delay,
                    respect_robots_txt: config.respect_robots_txt,
                    client: client.clone(),
                    skip_non_successful_response: config.skip_non_successful_response,
                    max_depth: config.max_depth.unwrap_or(usize::MAX),
                    max_requests,
                };
                allow_list.allow(domain, allow);
            }
            DomainListing::AllowList(allow_list)
        };

        Self {
            client,
            list,
            current_depth: 0,
            max_depth: config.max_depth.unwrap_or(usize::MAX),
            respect_robots_txt: config.respect_robots_txt, 
            skip_non_successful_response: config.skip_non_successful_response,
        }
    }

    // è·å–å…è®¸çš„æœ€å¤§è¯·æ±‚æ·±åº¦
    pub fn max_depth(&self) -> usize {
        self.max_depth
    }

    /// æ˜¯å¦éµå¾ªç›®æ ‡ç½‘ç«™çš„çˆ¬è™«è§„åˆ™
    pub fn respect_robots_txt(&self) -> bool {
        self.respect_robots_txt
    }

    /// æ˜¯å¦è·³è¿‡è®¿é—®ä¸æˆåŠŸçš„è¯·æ±‚å“åº”
    pub fn skip_non_successful_response(&self) -> bool {
        self.skip_non_successful_response
    }
}

struct RequestDelay {}

pub struct CrawlerConfig {
    /// è®¿é—®çš„è¯·æ±‚é€’å½’æ·±åº¦é™åˆ¶
    max_depth: Option<usize>,
    /// æœ€å¤§è¯·æ±‚æ•°, é»˜è®¤ä¸º MAX_CONCURRENT_REQUESTS
    max_requests: Option<usize>,
    /// è·³è¿‡å¤±è´¥è¯·æ±‚
    skip_non_successful_response: bool,
    /// ç™½åå•ï¼Œå¦‚æœä¸ºç©ºï¼Œä¸ä½œé™åˆ¶
    allowed_domains: HashMap<String, Option<RequestDelay>>,
    /// é»‘åå• ğŸš«
    disallowed_domains: HashSet<String>,
    /// æ˜¯å¦éµä»ç›®æ ‡ç½‘ç«™çš„æ•°æ®è·å–è§„åˆ™ 
    /// robots.txt: <http://www.robotstxt.org>
    respect_robots_txt: bool,
    /// æ‰§è¡Œ http è¯·æ±‚çš„å®¢æˆ·ç«¯
    client: Option<reqwest::Client>,
}
    
impl Default for CrawlerConfig {
    fn default() -> Self {
        Self {
            max_depth: None,
            max_request: None,
            skip_non_successful_response: true,
            allowed_domains: Default::default(),
            disallowed_domains: Default::default(),
            respect_robots_txt: false,
            client: None,
        }    
    }
}
            
impl CrawlerConfig {
    const MAX_CONCURRENT_REQUESTS: usize = 1_00;

    pub fn max_depth(mut self, max_depth: usize) -> Self {
        self.max_depth = Some(max_depth);
        self
    }

    pub fn respect_robots_txt(mut self) -> Self {
        self.respect_robots_txt = true;
        self
    }

    pub fn scrape_non_sucess_response(mut self) -> Self {
        self.skip_non_successful_response = false;
        self
    }

    pub fn max_current_requests(mut self, max_requests: usize) -> Self {
        self.max_requests = Some(max_requests);
        self
    }

    pub fn set_client(mut self, client: reqwest::Client) -> Self {
        self.client = Some(client);
        self
    }

    pub fn disallowed_domain(mut self, domain: impl Into<String>) -> Self {
        self.disallowed_domains.insert(domain);
        self
    }

    pub fn disallowed_domains<D, T>(mut self, domains: D) -> Self 
        where
            D: IntoIterator<Item = T>,
            T: Into<String>,
    {
        for domain in domains.into_iter() {
            self.disallowed_domains.insert(domain.into());
        }
        self
    }

    pub fn allow_domain(mut self, domain: impl Into<String>) -> Self {
        self.allowed_domains.insert(domain.into(), None);
        self
    }

    pub fn allow_domain_with_delay(mut self, 
        domain: impl Into<String>, 
        delay: RequestDelay,
    ) -> Self {
        self.allowed_domains.insert(domain, delay);
        self
    }

    pub fn allow_domains<D, T>(mut self, domains: D) -> Self 
        where
            D: IntoIterator<Item = T>,
            T: Into<String>,
    {
        for domain in domains.into_iter() {
            self.allowed_domains.insert(domain.into(), None)
        }

        self
    }

    pub fn allow_domains_with_delay<D, T>(mut self, domains: D) -> Self
        where
            D: IntoIterator<Item = (T, RequestDelay)>,
            T: Into<String>,
    {
        for (domain, deplay) in domains.into_iter() {
            self.allowed_domains.insert(domain.into(), deplay);
        }
        self
    }
}
