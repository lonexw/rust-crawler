use std::collections::{HashMap, HashSet};

pub trait Scraper {}

/// Crawler è´Ÿè´£æ‰§è¡Œéœ€è¦çˆ¬å–çš„ç½‘ç«™åœ°å€åˆ—è¡¨, 
/// å¹¶å°†æˆåŠŸçš„å“åº”ç»“æœè½¬å‘ç»™ Scraper å¤„ç†
pub struct Crawler<T: Scraper> {
    client: reqwest::Client,
}

impl<T: Scraper> Crawler<T> {
    /// ä» CrawlerConfig ä¸­åˆ›å»º Crawler å®ä¾‹
    pub fn new(config: CrawlerConfig) -> Self {
        let client = config.client.unwrap_or_default(); 

        Self {
            client: client,
        }
    }

    // è·å–å…è®¸çš„æœ€å¤§è¯·æ±‚æ·±åº¦
    pub fn max_depth(&self) -> usize {
        self.max_depth
    }

    /// æ˜¯å¦éµå¾ªç›®æ ‡ç½‘ç«™çš„çˆ¬è™«è§„åˆ™
    pub fn respect_robots_txt(&self) -> bool {
        self.respect_robots_txt
    }

    /// æ˜¯å¦è·³è¿‡è®¿é—®ä¸æˆåŠŸçš„è¯·æ±‚å“åº”
    pub fn skip_non_successful_response(&self) -> bool {
        self.skip_non_successful_response
    }
}

struct RequestDelay {}

pub struct CrawlerConfig {
    /// è®¿é—®çš„è¯·æ±‚é€’å½’æ·±åº¦é™åˆ¶
    max_depth: Option<usize>,
    /// æœ€å¤§è¯·æ±‚æ•°, é»˜è®¤ä¸º MAX_CONCURRENT_REQUESTS
    max_requests: Option<usize>,
    /// è·³è¿‡å¤±è´¥è¯·æ±‚
    skip_non_successful_response: bool,
    /// ç™½åå•ï¼Œå¦‚æœä¸ºç©ºï¼Œä¸ä½œé™åˆ¶
    allowed_domains: HashMap<String, Option<RequestDelay>>,
    /// é»‘åå• ğŸš«
    disallowed_domains: HashSet<String>,
    /// æ˜¯å¦éµä»ç›®æ ‡ç½‘ç«™çš„æ•°æ®è·å–è§„åˆ™ 
    /// robots.txt: <http://www.robotstxt.org>
    respect_robots_txt: bool,
    /// æ‰§è¡Œ http è¯·æ±‚çš„å®¢æˆ·ç«¯
    client: Option<reqwest::Client>,
}
    
impl Default for CrawlerConfig {
    fn default() -> Self {
        Self {
            max_depth: None,
            max_request: None,
            skip_non_successful_response: true,
            allowed_domains: Default::default(),
            disallowed_domains: Default::default(),
            respect_robots_txt: false,
            client: None,
        }    
    }
}
            
impl CrawlerConfig {
    const MAX_CONCURRENT_REQUESTS: usize = 1_00;

    pub fn max_depth(mut self, max_depth: usize) -> Self {
        self.max_depth = Some(max_depth);
        self
    }

    pub fn respect_robots_txt(mut self) -> Self {
        self.respect_robots_txt = true;
        self
    }

    pub fn scrape_non_sucess_response(mut self) -> Self {
        self.skip_non_successful_response = false;
        self
    }

    pub fn max_current_requests(mut self, max_requests: usize) -> Self {
        self.max_requests = Some(max_requests);
        self
    }

    pub fn set_client(mut self, client: reqwest::Client) -> Self {
        self.client = Some(client);
        self
    }

    pub fn disallowed_domain(mut self, domain: impl Into<String>) -> Self {
        self.disallowed_domains.insert(domain);
        self
    }

    pub fn disallowed_domains<D, T>(mut self, domains: D) -> Self 
        where
            D: IntoIterator<Item = T>,
            T: Into<String>,
    {
        for domain in domains.into_iter() {
            self.disallowed_domains.insert(domain.into());
        }
        self
    }

    pub fn allow_domain(mut self, domain: impl Into<String>) -> Self {
        self.allowed_domains.insert(domain.into(), None);
        self
    }

    pub fn allow_domain_with_delay(mut self, 
        domain: impl Into<String>, 
        delay: RequestDelay,
    ) -> Self {
        self.allowed_domains.insert(domain, delay);
        self
    }

    pub fn allow_domains<D, T>(mut self, domains: D) -> Self 
        where
            D: IntoIterator<Item = T>,
            T: Into<String>,
    {
        for domain in domains.into_iter() {
            self.allowed_domains.insert(domain.into(), None)
        }

        self
    }

    pub fn allow_domains_with_delay<D, T>(mut self, domains: D) -> Self
        where
            D: IntoIterator<Item = (T, RequestDelay)>,
            T: Into<String>,
    {
        for (domain, deplay) in domains.into_iter() {
            self.allowed_domains.insert(domain.into(), deplay);
        }
        self
    }
}